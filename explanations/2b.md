<h2>What I've done to perform XXS</h2>

There is no real sanitation of the input for what it put into the user's info section, as in noted with a "TODO"-tag:
![Pic showing the vulernable part of the app.py code](https://i.imgur.com/OMnPUYN.png)

I can therefore inject it with potentially malicious code.

I modified a simple example from SO:
https://stackoverflow.com/questions/7172732/redirect-outside-the-current-domain-using-javascript<br><br>
And add the following into the "About me" section:

![](https://i.imgur.com/MEFN2Vu.jpg)
```text
<img src="http://url.to.file.which/not.exist" onerror="window.location.href = 'https://www.google.com';">
```

<br>
Logged in as the user Bob, I put a fake embedded image in the about me section, but gave it a source which I know will result in an error. I catch this error by redirecting any user to a relatively evil website. I could, in theory, redirect them anywhere I wanted. Possibly a fake login page from where I could steal try their credentials, or any other malicious place.
<br>
<br>
When innocent Alice checks what her friend bob is up to these days, her browser window will redirect to the evil website, meaning that the injected code has been executed. Notice the "failed" image in bob's about me section, and that the browser is currently loading a page. I could have done a screen recording of this (as this is a bad demonstration, sorry, I thought it would be cool.) 

![](https://i.imgur.com/zGg6tif.png)
![](https://i.imgur.com/OMMBRYV.png)

Could also have logged something to do the console or done some other funny stuff.
<br><br><br>

<h2>Could we solve (some of) the injection problems with a restrictive Content Security Policy?</h2>
Based on the "guide" found at:
https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Content-Security-Policy/script-src
<br><br>
Yes. Starting out with a completely minimal policy and making a gradual whitelisting approach would allow you to control what users are allowed to achieve enter into their "About me" pages. Using CSP's nonce and hash-base approach would allow you to whitelist specific inline event handling, which would deter against a redirection from an onerror-statement, which would prevent that specific type of code execution. One can tailor the CSP to create a balance between user functionality and safety concerns.<br><br>

It's good to note that Google's analysis of "approximately 100 billion pages from over 1 billion hostnames" concluded that "94.68% of policies that attempt to limit script execution are ineffective, and that 99.34% of hosts with CSP use policies that offer no benefit against XSS", which is a reminder that CSP is not a magical catch-all defense system. Google recommends the usage of cryptographic nonces, and specifically criticizes domain-based whitelisting. Their findings can be found here:
https://research.google/pubs/pub45542/
 <br><br>

<h2>Try visiting /users/me in the browser. Does your code injection still work? Explain.</h2>

No it does not work. We "borrow security" from the jinja2 rendering already, which has good parameterization. There is also a CSP in place for user.html.
<br>

<h2>Fix the code injection problems you find in script.js</h2>

I've updated format_field and format_profile specifically to safely wrap its html code, taking inspiration from the demo code. This will deal with injections in the about-me section.<br><br>

You can still do damage with the other fields, though. I've further sanitised the input coming INTO script.js, that is, dealing with it directly in the form data of app.py. 
![](https://i.imgur.com/K4HRt8w.png)

You can still inject code into the image-url, I was not able to fix this. I have to use some external library and I never settled on one. Sorry.

<br>

